# Code architecture

At a high level, the program does only three things: 
1. it takes sentences inputted by the user, translates that into a "lower-level" form (in the programming sense—namely, the form is Z3 BitVectors and constraints)
2. does something with that lower level form (namely, solves it and organizes it)
3. translates that back into output that the user can interpret. 



## Step 1: User input to Z3
The first function that is called is `make_model_for`. When a set of premises, conclusions, and N is given to `make_model_for`, the first thing called is `prefix` from the `syntax` module. This converts the input, as a python string, into prefix notation. In prefix notation, every sentence is a python list, with the following rules:
- an atomic input sentence 'A' is [A] in prefix notation, where A is an AtomSort (declared Z3 sort—that will be important later)
- an atomic input sentence 'unary_operator A' is ['unary_operator', prefix(A)], where prefix(A) represents the prefix form of A (e.g. if A is atomic, 'unary_operator A' is ['unary_operator', [A]], where the A in the embedded list is an AtomSort).
- an atomic sentence 'A binary_operator B' is ['binary_operator', prefix(A), prefix(B)] (e.g. 'A \binary_operator B' where both A and B are atomic is ['binary_operator', [A], [B]], where the A and B in the embedded lists are AtomSorts).
This ensures that every prefix sentence is an instance of the python list class, and if the length of that list is larger than one, then the first element is the main operator of the sentence (as a string). This allows for easy access to recursive funcitons. [^note_on_backslashes]

[^note_on_backslashes]: Trivially, the function `add_backslashes_to_infix` adds two backslashes to every operator and the special `top` sentence such that printed out they will be LaTeX commands. Two backslashes are necessary because '\' is a special character in python so one instance next to e.g. 'neg' will not print '\neg' but rather 'eg' a line after everything else. This also ensures that, in the options where a user can search for a prefix sentence in a solved model, they can find it regardless of whether they originally inputted it with backslashes or not (and regardless of whether they do so in the search).

To finish the first step, the prefix sentences are used to make a ModelSetup object, which can be thought of as the space (I'll use a bucket as a metaphor) where everything having to do with the setup of the model is. It is inside this bucket that the semantics are defined and stored. Conceptually, the semantics (defined in the `semantics` module) are just a set of constraints, taken rather straightforwardly from the Counterfactuals manuscript, for the hyperintensional semantics. This is in some senses rather straight forward: it's just a series of Z3 constraints, which luckily read like logical sentences and follow from the manuscript (viz, quantified statements, etc). If you're not interested in how `semantics` module works, feel free to skip the next section. It is however probably of most interest to philosophers (which I am assuming many if not most readers are), as the name would suggest. 

### How does `semantics` work? 
We will take a slight detour from the `make_model_for` story to talk about how the semantics work. The most fundamental concept in the `semantics` module (and perhaps in the entirety of the package) is the Z3 `|` OR operator. Every state (look at the manuscript or Fine 2017 for what states are) is modeled as a Z3 BitVector. A BitVector is an integer represented in binary bits (e.g. 5 is represented as 101). This is convenient because the Z3 OR operator is a bitwise "or" operation on two BitVectors. For example, supposing 3 and 5 are both BitVectors (that is, integers in binary), (trivially, 3 | 5 =) 011 | 101 = 111 (= 7, trivially, but kind of interesting when what we've essentially done is reduce sentences to a system of equations). Since all relations in the manuscript are defined on the basis of the fusions of states—which states are compatible, possible, alternatives to others—this function basically allows for this project to use Z3 to solve constraints (i.e. this project is possible basically because of this function). BitVectors are used throughout the entire program _right until_ the moment of translation into a readable output—quite literally right until: whenever you see a function referring to states in the code, _never_ does it work with a printable/readable version of the state (e.g., 101 as 'a.c') (unless of course that function is going to print the state). Most notably, the notions of "possible" and "verify" and other similar ones now thus have a natural understanding as a mathematical (i.e., not a python function—I will use mathematical to refer to this sense of the word function; otherwise function will refer to the python sense unless context is obvious) function from numbers (viz. BitVectors i.e. states) to truth values (e.g. which ones verify others, which ones are possible). The constraints are thus definitions which determine _which states_ are map to True and which ones to False for each of these notions. With that in mind, a philosopher looking at the `semantics` module should be able to understand the functions `true_at`, `extended_verify`, `is_world`, `is_alternative`, etc without any coding background: just substitute any instance of BitVectors with states in your head, and the rest is right from the manuscript. Unfortunately the entirety of the module is not that simple because a sneaky programming trick had to be employed. Many of these functions depend on `N`, which is the total number of atomic states in the model that is desired. Because of this, they must have `N` as an input. However, since many of the functions are recursive and there are just so many functions overall adding `N` as an input proves tedious. So, instead, the semantics is defined inside the scope of another function, which takes `N` as an input. (It also takes `possible`, `verify`, and those other similar ones as input, since those also depend on `N` on the Z3 side of things) This means that no function (like e.g. `extended_verify`) needs to have `N` passed into it even though it needs it: the function simply would look to the next frame where the variable `N` is defined, which is the frame of the outer function so to speak (same goes for `possible`, `verify`, etc). This seems weird, but kind of has an intuitive reading: to define "all" of the states, we need to know how many states we're talking about, so in order to define the semantics, `N` has to be known. Fittingly, the outer function—the one _in which_ the semantics are literally defined and under this intuitive reading are conceptually defined—is called `define_N_semantics`.

When the premises, conclusions, and `N` are passed into `define_N_semantics`, constraints following the semantics in the manuscript are made. This are saved to the ModelSetup object. At this point, the first step is done: the sentences have been translated into a lower level form, and what we have to work with is now a set of constraints on a whole bunch of integers as to which are "possible," "verify" others, etc. 


## Step 2: Solving the Z3 Constraints (i.e., Getting a Model) and Organizing Results
You may think this step is the most complex in the code, but since we just use the power of Z3 it is actually the most simple—why build tools to solve a huge problem when you can instead just translate your problem into one that someone else already has the tools to solve? As of now we have a ModelSetup object: a bucket with everything we've used so far and (most importantly) a whole bunch of Z3 constraints (constraints on functions that map integers to truth values—see the section "How does `semantics` work?" for background on that). Once the ModelSetup object has been made, you can now solve it with its `solve` method, which is called inside of `make_model_for`. At a high level, in the function a Z3 Solver object is made, the constraints are added to it, Z3 does all the number crunching, and Z3 then spits out a model (a ModelRef object). This is a model that specifies the domain and mappings of the functions `possible`, `verify`, etc. Since all the definitions, like whether a world is an alternative to another, is defined in terms of these functions, we can also extract the domains and values of those definitions (thinking of them as mathematical funtions). (In reality this all happens inside a function called `solve_constraints`, defined in the `semantics` module, which is called inside of the `solve` method, but that is trivial). (Some more information, like the time it took to solve the model and whether a model was in fact found, is also returned.) With this done, returning to our `make_model_for` story, we make a `ModelStructure` object, return it, and we exit the function (notice–that single function is the user's indirect interation with the majority of the code in the package). 

The dirty work that remains is to extract that information about how states (so really integers in binary) relate to concepts like "possible", verification, alternate-ness, worldhood, etc. This is done with the `ModelStructure` and `StateSpace` objects in the `model_structure` module. The ModelStructure object is basically an updated bucket, for convenience, that stores everything that's relevant up to now. It's necessary because in case there is not a model, there is not a state space, so it's counterintuitive to make a StateSpace object. So in the case there is no model, `ModelStructure` is the final result. Otherwise, we proceed to make a `StateSpace` object, which takes the previous two buckets to set up a the space of states. Inside it you can find which states are possible, which are worlds, which is the main world of interpretation, the (mathematical) functions `possible`, `verify`, etc. Proposition objects are also made, which are essentially just a place to store the verifiers and falsifiers of every input sentence (and intermediate sentence). The verifiers and falsifiers for extensional sentences are rather straightforward (and follow the definitions in the manuscript); for non-extensional sentences, a true sentence has a verifier of the null state and no falsifiers, and the converse if false. These objects (themselves mini-buckets of information) are themselves stored in the bigger StateSpace bucket. At this point, all of the relevant information has been found from the Z3 constraints and organized—there is now a list of states (i.e. numbers) that are possible, that are worlds, etc. However, remember how we solved the constraints: in Z3. So everything at this point remains in terms of BitVectors, which are literally just numbers. This is very hard nigh impossible to understand as raw output, so we must translate it back into something that philsophers (or really anyone besides Z3) can understand. 


## Step 3: Translating from Z3 to Human-Readable Output







Step 3: 